{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-30T04:02:00.590852300Z",
     "start_time": "2023-04-30T04:02:00.338866200Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Main libraries used in Bayesian Optimisation Implementation using Gausian Process\n",
    "import pymc as pm\n",
    "from scipy.optimize import minimize, dual_annealing\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "class bayes_optimizer():\n",
    "    def __init__(self, objective_func, x_range, kernel='Matern52', mid_points=9, plot=True, mode=\"MCMC\"):\n",
    "        self.objective_func = objective_func  #Our blackbox function\n",
    "        self.cov_func = kernel  #Kernel to be used\n",
    "        # Parameters bounds to be used assuming the parameters are continuous\n",
    "        # Can easily be extended to discrete space by using any mapping\n",
    "        self.x_min = np.array(x_range[0]).reshape(-1,)\n",
    "        self.x_max = np.array(x_range[1]).reshape(-1,)\n",
    "        self.dim = len(self.x_min)\n",
    "\n",
    "        # Pickup mid_points initially between 0.1 - 0.9 of the parameter range\n",
    "        # Sampling the objective function uniformly at mid_points\n",
    "        self.x = self.x_min + np.linspace(0, 1, mid_points + 2) * (self.x_max - self.x_min)\n",
    "        self.x = np.reshape(self.x, (len(self.x), 1))\n",
    "        print(self.x)\n",
    "        self.y = np.array([objective_func(i) for i in self.x]).reshape(-1,)\n",
    "        self.if_plot = plot\n",
    "        self.best_values = []\n",
    "        self.mode = mode\n",
    "\n",
    "    def Sampling(self, mode=\"OPT\", acquisition=\"EI\"):\n",
    "        with pm.Model() as model:\n",
    "            # setting D+3 hyperparameters for Gaussian process by sampling it from Gamma distributions\n",
    "            # These parameters govern the prior distriburions\n",
    "            # How to get the best prior ??\n",
    "            theta = pm.Gamma(\"theta\", alpha=1, beta=1, shape=(self.dim,))\n",
    "            theta0 = pm.Gamma(\"theta0\", alpha=1, beta=1)\n",
    "            # Using ARD Matern52 as covariance function to be used\n",
    "            # Can experiment with other covariance functions\n",
    "            if self.cov_func == \"Matern52\":\n",
    "                cov_func = theta0 ** 2 * pm.gp.cov.Matern52(input_dim=self.dim, ls=theta)\n",
    "            # We will use constant mean for our Gaussian process\n",
    "            m = pm.gp.mean.Constant(self.y.mean())\n",
    "            gp = pm.gp.Marginal(mean_func=m, cov_func=cov_func)\n",
    "            # For nu, using a Half-Cauchy distribution for noise\n",
    "            nu = pm.HalfCauchy(\"nu\", 1)\n",
    "            gp.marginal_likelihood(\"f\", X=self.x, y=self.y, noise=nu)\n",
    "\n",
    "            if mode == \"MCMC\":\n",
    "                N_trace = 1000 #Number of samples drawn\n",
    "                thinning = 4 # Thinning factor\n",
    "                trace = pm.sample(N_trace,chains=1)\n",
    "                pm.plot_trace(trace)\n",
    "                # Create multitrace object to pass after thinning\n",
    "                theta_list = np.array(trace['posterior']['theta'].sel(draw=slice(0,N_trace-1,int(N_trace/thinning)))[0])\n",
    "                theta0_list = np.array(trace['posterior']['theta0'].sel(draw=slice(0,N_trace-1,int(N_trace/thinning)))[0])\n",
    "                nu_list = np.array(trace['posterior']['nu'].sel(draw=slice(0,N_trace-1,int(N_trace/thinning)))[0])\n",
    "                trace_list = []\n",
    "                for i in range(len(theta_list)):\n",
    "                  trace_list.append({'theta': theta_list[i], 'theta0': theta0_list[i], 'nu': nu_list[i]})\n",
    "                x_next = self.Get_next(params=trace_list,acquisition=acquisition,gp=gp)\n",
    "                return x_next\n",
    "            elif mode == \"OPT\":\n",
    "                # Optimisation method\n",
    "                MAP = pm.find_MAP()\n",
    "                x_next = self.Get_next(params=MAP, acquisition=acquisition, gp=gp)\n",
    "                return x_next\n",
    "\n",
    "    def Get_next(self, params, acquisition, gp):\n",
    "        # define acqusition function\n",
    "        def acq(x_star):\n",
    "            # print(f\"Debug: {x_star}\" , len(x_star))\n",
    "            x_star = np.array(x_star).reshape(1, -1)\n",
    "            if self.mode == 'MCMC':\n",
    "                integrated_acq = 0\n",
    "                for t in params:\n",
    "                    mean, var = gp.predict(x_star, point=t, diag=True, pred_noise=False)\n",
    "                    mean = mean[0]\n",
    "                    std = np.sqrt(var[0])\n",
    "                    if acquisition == \"EI\":\n",
    "                        #Expected Improvement\n",
    "                        f_best = self.y.min()\n",
    "                        gamma = (-f_best + mean) / std\n",
    "                        integrated_acq = integrated_acq +  (std * (norm.cdf(gamma) * gamma + norm.pdf(gamma)))\n",
    "                    elif acquisition == \"PI\":\n",
    "                        # Probability of improvement\n",
    "                        f_best = self.y.min()\n",
    "                        gamma = (-f_best + mean) / std\n",
    "                        integrated_acq = integrated_acq + (norm.cdf(gamma))\n",
    "                    elif acquisition == \"LCB\":\n",
    "                        # Tunable parameter Kappa in case of Lower Confidence bound\n",
    "                        kappa = 1\n",
    "                        integrated_acq = integrated_acq + (mean - (kappa) * std)\n",
    "                integrated_acq = integrated_acq/len(params)\n",
    "                return integrated_acq\n",
    "            else: #OPT mode where there is no integration over various samples\n",
    "                mean, var = gp.predict(x_star, point=params, diag=True, pred_noise=False)\n",
    "                mean = mean[0]\n",
    "                std = np.sqrt(var[0])\n",
    "                if acquisition == \"EI\":\n",
    "                    #Expected Improvement\n",
    "                    f_best = self.y.min()\n",
    "                    gamma = (-f_best + mean) / std\n",
    "                    return std * (norm.cdf(gamma) * gamma + norm.pdf(gamma))\n",
    "                elif acquisition == \"PI\":\n",
    "                    # Probability of improvement\n",
    "                    f_best = self.y.min()\n",
    "                    gamma = (-f_best + mean) / std\n",
    "                    return norm.cdf(gamma)\n",
    "                elif acquisition == \"LCB\":\n",
    "                    # Tunable parameter Kappa in case of Lower Confidence bound\n",
    "                    kappa = 1\n",
    "                    return mean - (kappa) * std\n",
    "\n",
    "        # optimize for the best point from acquisition function\n",
    "        bounds = [(low, high) for low, high in zip(self.x_min, self.x_max)]\n",
    "        alpha_temp = np.random.rand(1)[0]\n",
    "        result = minimize(acq, x0=alpha_temp*self.x_min + (1-alpha_temp)*self.x_max, bounds=bounds)\n",
    "        # result = dual_annealing(acq, bounds=bounds)\n",
    "\n",
    "        #Debug start\n",
    "        # debug_x = np.linspace(0, 1, 40)*(self.x_max - self.x_min) + self.x_min\n",
    "        # debug_y = []\n",
    "        # for i in debug_x:\n",
    "        #     debug_y.append(acq(i))\n",
    "        # plt.figure()\n",
    "        # plt.plot(debug_x, debug_y)\n",
    "        # plt.show()\n",
    "        #Debug end\n",
    "        x_next = result.x\n",
    "        print(f\"x_next = {x_next}\")\n",
    "        # ipdb.set_trace()\n",
    "        return x_next\n",
    "\n",
    "    def find_best(self, mode=\"MCMC\", acquisition=\"EI\", max_iter=10):\n",
    "        for i in range(max_iter):\n",
    "            print(f'----Iteration{i} started')\n",
    "            self.best_values.append(np.min(self.y))\n",
    "            x_new = self.Sampling(acquisition=acquisition, mode=mode)\n",
    "            # if x_new in self.x:\n",
    "            #     break\n",
    "            y_new = self.objective_func(x_new)\n",
    "            def plot_objective(self, func, point):\n",
    "                x = np.linspace(self.x_min, self.x_max, 25)\n",
    "                y = func(x)\n",
    "                plt.figure()\n",
    "                plt.plot(x, y, 'g--', label=\"Blackbox function\")\n",
    "                plt.scatter(self.x, self.y, marker='o', color=\"b\", label=\"Existing pts\")\n",
    "                plt.plot([point], [func(point)], marker='o', markersize=10, color=\"red\", label=\"Next pt\")\n",
    "                plt.legend(loc=\"best\")\n",
    "                plt.show()\n",
    "            if self.if_plot:\n",
    "                # Plot the objective/blackbox function with current values and next value\n",
    "                # Can be used only in case of 1-D plots\n",
    "                plot_objective(self, self.objective_func, x_new)\n",
    "            self.x = np.vstack((self.x, x_new))\n",
    "            self.y = np.hstack((self.y, y_new))\n",
    "        return self.x[np.argmin(self.y)], self.best_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.        ]\n",
      " [ 1.25663706]\n",
      " [ 2.51327412]\n",
      " [ 3.76991118]\n",
      " [ 5.02654825]\n",
      " [ 6.28318531]\n",
      " [ 7.53982237]\n",
      " [ 8.79645943]\n",
      " [10.05309649]\n",
      " [11.30973355]\n",
      " [12.56637061]]\n",
      "----Iteration0 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\envs\\scientific\\lib\\site-packages\\pymc\\gp\\gp.py:54: FutureWarning: The 'noise' parameter has been been changed to 'sigma' in order to standardize the GP API and will be deprecated in future releases.\n",
      "  warnings.warn(_noise_deprecation_warning, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Minimizing Griewank function of 1 dimension\n",
    "g = lambda x:(x**2)/100 + (np.cos(x))\n",
    "g_range = [0, 4*np.pi]\n",
    "mid_points = 9\n",
    "max_iter = 20\n",
    "baye_1d_GP_OPT = bayes_optimizer(objective_func=g,x_range=g_range,mid_points=mid_points, mode=\"MCMC\")\n",
    "best_result, best_val_mcmc_ei = baye_1d_GP_OPT.find_best(mode = \"MCMC\",acquisition = \"EI\",max_iter = max_iter)\n",
    "\n",
    "baye_1d_GP_OPT = bayes_optimizer(objective_func=g,x_range=g_range,mid_points=mid_points, mode=\"MCMC\")\n",
    "best_result, best_val_mcmc_pi = baye_1d_GP_OPT.find_best(mode = \"MCMC\",acquisition = \"PI\",max_iter = max_iter)\n",
    "\n",
    "baye_1d_GP_OPT = bayes_optimizer(objective_func=g,x_range=g_range,mid_points=mid_points, mode=\"MCMC\")\n",
    "best_result, best_val_mcmc_lcb = baye_1d_GP_OPT.find_best(mode = \"MCMC\",acquisition = \"LCB\",max_iter = max_iter)\n",
    "\n",
    "baye_1d_GP_OPT = bayes_optimizer(objective_func=g,x_range=g_range,mid_points=mid_points, mode=\"OPT\")\n",
    "best_result, best_val_opt_ei = baye_1d_GP_OPT.find_best(mode = \"OPT\",acquisition = \"EI\",max_iter = max_iter)\n",
    "\n",
    "baye_1d_GP_OPT = bayes_optimizer(objective_func=g,x_range=g_range,mid_points=mid_points, mode=\"OPT\")\n",
    "best_result, best_val_opt_pi = baye_1d_GP_OPT.find_best(mode = \"OPT\",acquisition = \"PI\",max_iter = max_iter)\n",
    "\n",
    "baye_1d_GP_OPT = bayes_optimizer(objective_func=g,x_range=g_range,mid_points=mid_points, mode=\"OPT\")\n",
    "best_result, best_val_opt_lcb = baye_1d_GP_OPT.find_best(mode = \"OPT\",acquisition = \"LCB\",max_iter = max_iter)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.arange(max_iter), best_val_mcmc_ei, label='MCMC EI')\n",
    "plt.plot(np.arange(max_iter), best_val_mcmc_pi, label='MCMC PI')\n",
    "plt.plot(np.arange(max_iter), best_val_mcmc_lcb, label='MCMC LCB')\n",
    "plt.plot(np.arange(max_iter), best_val_opt_ei, label='OPT EI')\n",
    "plt.plot(np.arange(max_iter), best_val_opt_pi, label='OPT PI')\n",
    "plt.plot(np.arange(max_iter), best_val_opt_lcb, label='OPT LCB')\n",
    "plt.title('Comparison in minimizing Griewank function')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
